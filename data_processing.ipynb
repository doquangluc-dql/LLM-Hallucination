{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T08:02:15.185389Z",
     "iopub.status.busy": "2025-09-27T08:02:15.185061Z",
     "iopub.status.idle": "2025-09-27T08:02:28.533347Z",
     "shell.execute_reply": "2025-09-27T08:02:28.532200Z",
     "shell.execute_reply.started": "2025-09-27T08:02:15.185351Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "# !pip install transformers==4.22.2 datasets==2.4.0 tokenizers==0.12.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T08:05:16.121790Z",
     "iopub.status.busy": "2025-09-27T08:05:16.121394Z",
     "iopub.status.idle": "2025-09-27T08:05:16.128407Z",
     "shell.execute_reply": "2025-09-27T08:05:16.127431Z",
     "shell.execute_reply.started": "2025-09-27T08:05:16.121759Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T08:03:12.609960Z",
     "iopub.status.busy": "2025-09-27T08:03:12.608784Z",
     "iopub.status.idle": "2025-09-27T08:03:19.263969Z",
     "shell.execute_reply": "2025-09-27T08:03:19.263060Z",
     "shell.execute_reply.started": "2025-09-27T08:03:12.609887Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"NlpHUST/vi-word-segmentation\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"NlpHUST/vi-word-segmentation\")\n",
    "\n",
    "nlp = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T08:03:46.613854Z",
     "iopub.status.busy": "2025-09-27T08:03:46.612716Z",
     "iopub.status.idle": "2025-09-27T08:03:46.623817Z",
     "shell.execute_reply": "2025-09-27T08:03:46.622653Z",
     "shell.execute_reply.started": "2025-09-27T08:03:46.613809Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_segment(example):\n",
    "    # Khối try chứa code bạn muốn thực thi\n",
    "    try:\n",
    "        # 1. Gọi mô hình NLP (Đây là nơi có khả năng xảy ra lỗi)\n",
    "        ner_results = nlp(example)\n",
    "        \n",
    "        example_tok = \"\"\n",
    "        \n",
    "        # 2. Duyệt qua kết quả và xây dựng chuỗi\n",
    "        for e in ner_results:\n",
    "            # Xử lý các token con (sub-words) thường có \"##\"\n",
    "            if \"##\" in e[\"word\"]:\n",
    "                example_tok = example_tok + e[\"word\"].replace(\"##\",\"\")\n",
    "            # Xử lý các thực thể tiếp theo trong chuỗi (Thực thể \"I\" - Inside)\n",
    "            elif e[\"entity\"] == \"I\":\n",
    "                example_tok = example_tok + \"_\" + e[\"word\"]\n",
    "            # Xử lý các token khác\n",
    "            else:\n",
    "                example_tok = example_tok + \" \" + e[\"word\"]\n",
    "        \n",
    "        # Nếu mọi thứ chạy thành công, trả về chuỗi đã xử lý\n",
    "        return example_tok.strip() # Dùng .strip() để loại bỏ khoảng trắng thừa ở đầu/cuối\n",
    "    \n",
    "    # Khối except sẽ bắt mọi lỗi xảy ra trong khối try\n",
    "    except Exception as error:\n",
    "        # Bạn có thể in lỗi ra để gỡ lỗi (debugging) nếu cần\n",
    "        print(f\"Lỗi khi xử lý: {example}. Chi tiết lỗi: {error}\")\n",
    "        \n",
    "        # Trả về giá trị đầu vào ban đầu theo yêu cầu\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T08:03:52.778266Z",
     "iopub.status.busy": "2025-09-27T08:03:52.777196Z",
     "iopub.status.idle": "2025-09-27T08:03:52.790521Z",
     "shell.execute_reply": "2025-09-27T08:03:52.789265Z",
     "shell.execute_reply.started": "2025-09-27T08:03:52.778234Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "special_case = {'dđ': 'đ'}\n",
    "not_in_vietnamese = ['w', 'f', 'j', 'z', 'W', 'F', 'J', 'Z']\n",
    "\n",
    "trap_in_prompt = [\n",
    "    'Adversarial Question: ',\n",
    "    'Noisy Question: ',\n",
    "    'đây là câu hỏi đã được gài bay: ',\n",
    "    'câu hỏi gài bẫy có thể như sau: \\n\\n'\n",
    "    'câu hỏi gài bẫy có thể là: '\n",
    "    'câu hỏi gài bẫy: '\n",
    "    'Câu hỏi gài bẫy: ',\n",
    "]\n",
    "\n",
    "INVALID = 'aaaaa'\n",
    "\n",
    "def remove_diacritics(text: str) -> str:\n",
    "    nfkd = unicodedata.normalize('NFD', text)\n",
    "    return ''.join(ch for ch in nfkd if unicodedata.category(ch) != 'Mn')\n",
    "\n",
    "def replace_word(sentence: str, old: str, new: str) -> str:\n",
    "    words = sentence.split()\n",
    "    replaced = [new if w == old else w for w in words]\n",
    "    return \" \".join(replaced)\n",
    "\n",
    "def modify_word(w):\n",
    "    \"\"\"\n",
    "    Sửa lỗi các từ như \"dđ\" và các ký tự lặp lại (ví dụ: \"haaa\" thành \"ha\").\n",
    "    Hàm này chỉ xử lý phần chữ cái của từ.\n",
    "    \"\"\"\n",
    "    if len(w) == 0:\n",
    "        return w\n",
    "    \n",
    "    # Xử lý các ký tự lặp lại\n",
    "    if w[0].islower():\n",
    "        for c in string.ascii_lowercase:\n",
    "            while c + c in w:\n",
    "                w = w.replace(c + c, c)\n",
    "        for c in not_in_vietnamese:\n",
    "            w = w.replace(c, '')\n",
    "    \n",
    "    # Xử lý các trường hợp đặc biệt\n",
    "    for old, new in special_case.items():\n",
    "        if old in w:\n",
    "            w = w.replace(old, new)\n",
    "            \n",
    "    return w\n",
    "\n",
    "def fix_sentence(s):\n",
    "    \"\"\"\n",
    "    Sửa lỗi từng từ trong một câu trong khi vẫn giữ lại dấu câu.\n",
    "    \"\"\"\n",
    "    # Tìm tất cả các từ (chuỗi chữ cái) và áp dụng hàm modify_word cho chúng\n",
    "    # Mọi thứ khác (dấu câu, khoảng trắng) sẽ được giữ nguyên\n",
    "    return re.sub(r'[a-zA-ZÀ-ỹ]+', lambda m: modify_word(m.group(0)), s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('./data/vihallu-train.csv',encoding='utf-8')\n",
    "# df['prompt'] = df['prompt'].apply(fix_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T08:04:27.037550Z",
     "iopub.status.busy": "2025-09-27T08:04:27.035405Z",
     "iopub.status.idle": "2025-09-27T08:04:27.052133Z",
     "shell.execute_reply": "2025-09-27T08:04:27.050732Z",
     "shell.execute_reply.started": "2025-09-27T08:04:27.037379Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def processing_data(input_file, output_file):\n",
    "    df = pd.read_csv(input_file,encoding='utf-8')\n",
    "    df['prompt'] = df['prompt'].apply(fix_sentence)\n",
    "    \n",
    "    for i, (c, p, r) in tqdm(enumerate(zip(df['context'], df['prompt'], df['response']))):\n",
    "        for trap in trap_in_prompt:\n",
    "            p = p.replace(trap, '').strip()\n",
    "        \n",
    "        p = get_segment(p)\n",
    "        c = get_segment(c)\n",
    "        r = get_segment(r)\n",
    "        \n",
    "        new_response = remove_diacritics(r)\n",
    "        \n",
    "        response_dicts = {}\n",
    "    \n",
    "        for w, new_w in zip(r.split(' '), new_response.split(' ')):\n",
    "            if response_dicts.get(new_w) is None:\n",
    "                response_dicts[new_w] = w\n",
    "                \n",
    "            elif response_dicts[new_w] != w:\n",
    "                response_dicts[new_w] = INVALID\n",
    "            \n",
    "        new_context = remove_diacritics(c)\n",
    "    \n",
    "        for w, new_w in zip(c.split(' '), new_context.split(' ')):\n",
    "            if response_dicts.get(new_w) is None:\n",
    "                response_dicts[new_w] = w\n",
    "            elif response_dicts[new_w] != w:\n",
    "                response_dicts[new_w] = INVALID\n",
    "    \n",
    "        new_prompt = remove_diacritics(p)\n",
    "        \n",
    "        for w, new_w in zip(p.split(' '), new_prompt.split(' ')):\n",
    "            if response_dicts.get(new_w) is not None and response_dicts.get(new_w) != INVALID:\n",
    "    \n",
    "                new_prompt = replace_word(new_prompt, new_w, response_dicts.get(new_w))\n",
    "            else:\n",
    "                new_prompt = replace_word(new_prompt, new_w, w)\n",
    "        df.loc[i, 'prompt'] = new_prompt\n",
    "    df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    print(\"Exported: \",output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T08:05:23.855881Z",
     "iopub.status.busy": "2025-09-27T08:05:23.855161Z",
     "iopub.status.idle": "2025-09-27T08:06:45.317232Z",
     "shell.execute_reply": "2025-09-27T08:06:45.315968Z",
     "shell.execute_reply.started": "2025-09-27T08:05:23.855849Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "processing_data('/kaggle/input/dsc25-data/vihallu-train.csv', 'vihallu-train-processed-3.csv')\n",
    "processing_data('/kaggle/input/dsc25-data/vihallu-public-test.csv', 'vihallu-public-test-processed-3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #In từ điểm ra xem vui vui th\n",
    "# s = set()\n",
    "\n",
    "# prompt = df['prompt']\n",
    "# for p in prompt:\n",
    "#     #tach cac tu cua p ra roi dua vao set\n",
    "#     words = p.split(' ')\n",
    "#     for w in words:\n",
    "#         s.add(w)\n",
    "\n",
    "# print(len(s))\n",
    "\n",
    "# dicts = {}\n",
    "# for w in s:\n",
    "#     dicts[w] = 0\n",
    "\n",
    "# for p in prompt:\n",
    "#     words = p.split(' ')\n",
    "    \n",
    "#     for w in words:\n",
    "#         if w in dicts:\n",
    "#             dicts[w] += 1\n",
    "\n",
    "# dicts = dict(sorted(dicts.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# with open('vocab.csv', 'w', encoding='utf-8') as f:\n",
    "#     for w in dicts:\n",
    "#         f.write(f'{w},{dicts[w]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8356312,
     "sourceId": 13186380,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
